{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2c32eb",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ca1fc",
   "metadata": {},
   "source": [
    "## Overall Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462c7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json, os\n",
    "\n",
    "from data_loader import FoodDataset\n",
    "from utils import get_transforms, load_model_and_stats, evaluate_model\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = get_transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7cc50",
   "metadata": {},
   "source": [
    "Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543415e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model2_metadata.json\", \"r\") as f:\n",
    "    meta2 = json.load(f)\n",
    "\n",
    "test_indices = meta2[\"test_indices\"]\n",
    "target_mean_raw = meta2[\"target_mean\"]\n",
    "target_std_raw = meta2[\"target_std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59078e24",
   "metadata": {},
   "source": [
    "Load data (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f611479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset size: 2249\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Subset(FoodDataset(\n",
    "    csv_path = \"../data/cleaned-food-data.csv\",\n",
    "    img_dir   = \"../data/data/images\",\n",
    "    transform = transform,\n",
    "    target_mean = target_mean_raw,\n",
    "    target_std  = target_std_raw\n",
    "),\n",
    "test_indices)\n",
    "\n",
    "print(\"Eval dataset size:\", len(test_dataset))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db787eb9",
   "metadata": {},
   "source": [
    "Load model using helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed140152",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2, mean2, std2 = load_model_and_stats(\n",
    "    \"../models/model_2.pt\",\n",
    "    \"../models/model2_metadata.json\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb5513",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d748d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 71/71 [02:44<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 163.14676107606624\n",
      "MSE: 66862.51286292664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model2, test_loader, mean2, std2, device=device)\n",
    "print(\"MAE:\", results[\"MAE\"])\n",
    "print(\"MSE:\", results[\"MSE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb26d69",
   "metadata": {},
   "source": [
    "## Per Output Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bded92a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-target errors (Model 2, test set):\n",
      "portion_total_g   MAE =    82.90   RMSE =   136.63\n",
      "calories_kcal     MAE =    99.48   RMSE =   163.52\n",
      "protein_g         MAE =     6.86   RMSE =    11.17\n",
      "fat_g             MAE =     5.68   RMSE =     9.99\n",
      "carbohydrate_g    MAE =    14.21   RMSE =    21.69\n"
     ]
    }
   ],
   "source": [
    "target_mean = torch.tensor(target_mean_raw, dtype=torch.float32, device=device)\n",
    "target_std  = torch.tensor(target_std_raw,  dtype=torch.float32, device=device)\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, dish_ids, targets in test_loader:\n",
    "\n",
    "        # Load data\n",
    "        images = images.to(device)\n",
    "        dish_ids = dish_ids.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Predict nutrition\n",
    "        preds_norm = model2(images, dish_ids)\n",
    "\n",
    "        # Denormalize\n",
    "        preds_denorm = preds_norm * target_std + target_mean\n",
    "        targets_denorm = targets * target_std + target_mean\n",
    "\n",
    "        all_preds.append(preds_denorm.cpu().numpy())\n",
    "        all_targets.append(targets_denorm.cpu().numpy())\n",
    "\n",
    "# Stack predictions and targets\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_targets = np.vstack(all_targets)\n",
    "\n",
    "# Calculate per target MAE and RMSE\n",
    "mae_per_target = np.mean(np.abs(all_preds - all_targets), axis = 0)\n",
    "rmse_per_target = np.sqrt(np.mean((all_preds - all_targets) ** 2, axis = 0))\n",
    "\n",
    "target_names = [\"portion_total_g\", \"calories_kcal\", \"protein_g\", \"fat_g\", \"carbohydrate_g\"]\n",
    "\n",
    "print(\"\\nPer-target errors (Model 2, test set):\")\n",
    "for name, mae, rmse in zip(target_names, mae_per_target, rmse_per_target):\n",
    "    print(f\"{name:16s}  MAE = {mae:8.2f}   RMSE = {rmse:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1508a54",
   "metadata": {},
   "source": [
    "Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6a9de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation results to ../outputs/model2_eval.json\n"
     ]
    }
   ],
   "source": [
    "# Save path\n",
    "save_path = \"../outputs/model2_eval.json\"\n",
    "\n",
    "# Convert to floats\n",
    "mae_per_target_list  = [float(x) for x in mae_per_target]\n",
    "rmse_per_target_list = [float(x) for x in rmse_per_target]\n",
    "\n",
    "# Output format\n",
    "results_dict = {\n",
    "    \"overall_MAE\": results[\"MAE\"],\n",
    "    \"overall_MSE\": results[\"MSE\"],\n",
    "    \"per_target\": {\n",
    "        name: {\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse\n",
    "        }\n",
    "        for name, mae, rmse in zip(target_names, mae_per_target_list, rmse_per_target_list)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save file\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(results_dict, f, indent=4)\n",
    "\n",
    "print(f\"Saved evaluation results to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8822efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
