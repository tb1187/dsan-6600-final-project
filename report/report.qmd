---
title: "Personalized Nutritional Recommendations from Food Images Using a Multimodal GUI"
author: "Chase Clemence & Tyler Blue"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
format:
  pdf:
    toc: true
    number-sections: true
bibliography: references.bib
link-citations: true
---

# Abstract

Accurate dieting requires manual input from the individual, requiring them to carefully measure the portion, macros, and quality of their food. Recent developments in the deep learning space attempt to automate this process, leveraging convolutional neural network (CNN) architectures to predict food types with semantic image segmentation as well as food volumne using pixel distances from the camera. However, no one has accurately solved this issue yet. We add the body of research by building a multimodal graphical user interface (GUI) that leverages a CNN to predict portion sizes and macros as well as a LLM wrapper that gives personalized dietary advice based on the food's content. The CNN is trained on the MM-Food-100K dataset - a state of the art food dataset contianing information about ingredients used, portion size, nutritional profile, cooking method, and more. The developed CNN used ResNet18 pretrained weights, a regression head size of 512, and a dropout rate of __. The LLM wrapper used leverages OpenAI's __ model to generate personalized dietary advice based on food macros, physical characteristics, and dietary goals. This is integrated into a simple GUI that consolodates the process into a simple, easy to use platform. The trained model performed well, achieving a MSE of __. With this GUI, users are able to get real-time improvements to their diet, helping them achieve their goals faster.

# 1. Introduction

Automated food recognition and nutritional estimation are emerging research areas with potential to substantially improve public health. Today, accurately measuring dietary macronutrients still requires individuals to manually weigh ingredients, look up nutritional values, and log foods by hand. This process is time-consuming, error-prone, and often frustrating, causing many people to abandon nutrition tracking altogether. If automated solutions become widely accessible, dietary monitoring could become far more efficient, lowering the barrier to healthy eating and enabling more consistent long-term adherence.

Deep learning — particularly convolutional neural networks (CNNs) — provides a promising pathway toward this goal. CNNs excel at extracting latent visual features from images, giving them the capacity to identify dishes, estimate portion sizes, and infer nutritional content directly from pixel information. When paired with an intuitive user interface, such systems could offer real-time macro predictions from a single food photograph, drastically simplifying the nutrition-tracking workflow.

Motivated by these opportunities, our project aims to develop a comprehensive, Python-based graphical user interface (GUI) capable of predicting portion size along with protein, fat, and carbohydrate content using a CNN trained on images of food. The GUI also integrates a large language model (LLM) to provide personalized feedback based on the predicted macros. Together, these components form a prototype system for fast, automated nutritional assessment.


# 2. Related Work



Summarize 3–6 works related to:
- Food image classification datasets  
- Food portion estimation  
- CNNs for regression  
- GUI-based ML applications  

Example inline citations:  
Food recognition approaches have advanced significantly in recent years @bossard2014food101.  
Image-based nutrition estimation remains an active research area @myers2015im2calories.

# 3. Methods

## 3.1 Dataset and Preprocessing
Explain:
- Data source (your cleaned-food-data CSV + images)  
- Portion/macro columns  
- Train/val/test split  
- Preprocessing pipeline  
- Dish ID embedding (and decision to remove it for unseen foods)

## 3.2 CNN Architecture



## 3.3 Training Procedure



# 4. GUI System Design



# 5. Results



# 6. Discussion & Future Work